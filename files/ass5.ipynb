{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "08c5ed94-5ead-45c5-be0d-6ea9398e98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for testing\n",
    "sample_text = \"\"\"\n",
    "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do:\n",
    "once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in i\n",
    "'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4f5c91d4-9939-4f6f-977b-6a296b57ca08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nalice was beginning to get very tired of sitting by her sister on the bank and of having nothing to do\\nonce or twice she had peeped into the book her sister was reading but it had no pictures or conversations in i\\nand what is the use of a book thought alice without pictures or conversation\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Special Characters, Punctuation, and Digits\n",
    "import re\n",
    "text = re.sub(r'[^A-Za-z\\s]', '', sample_text)\n",
    "\n",
    "#Convert to lowercase\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44bc15af-3e07-4ba5-af28-6f05acd9343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', 'and', 'of', 'having', 'nothing', 'to', 'do', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'i', 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', 'thought', 'alice', 'without', 'pictures', 'or', 'conversation']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Document into Sentences and Words\n",
    "sentences = text.split('.')\n",
    "\n",
    "# Tokenize each sentence into words\n",
    "tokenize_sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "print(tokenize_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d833f72b-c833-4cba-941f-e6eac54176a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['alice', 'beginning', 'get', 'tired', 'sitting', 'sister', 'bank', 'nothing', 'twice', 'peeped', 'book', 'sister', 'reading', 'pictures', 'conversations', 'use', 'book', 'thought', 'alice', 'without', 'pictures', 'conversation']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Remove stop words from the tokenized sentences\n",
    "\n",
    "filtered_sentences = [[word for word in sentence if word not in stop_words] for sentence in tokenize_sentences] \n",
    "print(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06494d63-4743-4b6f-9785-427d599cbe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CBOW Model (Using Gensim Word2Vec)\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train the Word2Vec model using CBOW (default is CBOW if sg=0)\n",
    "model = Word2Vec(sentences=filtered_sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Save trained model\n",
    "model.save('cbow_model_sample.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "13d3a4b2-693d-4bb8-9f0f-8024e8a61f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to alice is :  [('thought', 0.21885764598846436), ('bank', 0.21617142856121063), ('peeped', 0.0931011214852333), ('tired', 0.09291722625494003), ('without', 0.07964925467967987), ('conversation', 0.0628441646695137), ('use', 0.05435018613934517), ('sitting', 0.02706393599510193), ('get', 0.01611921563744545), ('pictures', -0.010837165638804436)]\n",
      "\n",
      "Predicted words for context: [('use', 0.055557035), ('beginning', 0.05555604), ('pictures', 0.055555962), ('tired', 0.055555895), ('get', 0.05555586), ('conversations', 0.05555586), ('sister', 0.05555585), ('book', 0.055555843), ('alice', 0.05555575), ('conversation', 0.055555705)]\n"
     ]
    }
   ],
   "source": [
    "# Load trained model\n",
    "\n",
    "loaded_model = Word2Vec.load('cbow_model_sample.model')\n",
    "\n",
    "# Find the most similar word to a specific word (eg. alice)\n",
    "similar_words = loaded_model.wv.most_similar('alice', topn=10)\n",
    "\n",
    "print('Most similar words to alice is : ', similar_words)\n",
    "\n",
    "# Predict output word based on context word\n",
    "\n",
    "context_words = ['sitting', 'sister']\n",
    "predicted_words = loaded_model.predict_output_word(context_words, topn=10)\n",
    "print(\"\\nPredicted words for context:\", predicted_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
